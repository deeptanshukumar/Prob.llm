prob_lm_assistant/
├── main.py                     # Main entry point, orchestrates the application
├── requirements.txt            # All Python dependencies
├── uploaded_docs/              # Folder for user-uploaded documents (and dummy files)
├── chroma_data/                # Persistent storage for ChromaDB
├── temp_images/                # Temporary storage for extracted images
└── core/                       # Contains core logic modules
    ├── __init__.py             # Makes 'core' a Python package
    ├── document_loaders.py     # Functions for loading/processing different document types
    ├── text_processing.py      # Functions for chunking text
    ├── vector_store.py         # Functions for ChromaDB initialization and management
    ├── retrieval.py            # Functions for BM25, Semantic, Hybrid retrieval
    └── llm_interface.py        # Functions for LLM interaction and RAG chain setup


Loading PDFs, Word documents, and standalone images (with OCR).
Chunking all this content.
Embedding it using nomic-embed-text via Ollama.
Storing it in ChromaDB.
Performing Hybrid Search (BM25 + Semantic Search with RRF).
And using your local Llama 3.2:1b LLM (via Ollama) to generate accurate, grounded, and more conversational answers based on the retrieved context.